{
  "platform_specific_arxiv_request": {
    "request_id": "platform_specific_arxiv_request",
    "parameters": {
      "num_queries": 2,
      "papers_per_query": 3,
      "num_papers_to_return": 2
    },
    "top_papers": [
      {
        "title": "Xception: Deep learning with depthwise separable convolutions",
        "authors": [
          "Chollet F."
        ],
        "year": 2017,
        "score": 0.9714285714285714,
        "extraction_result": null,
        "exclusion_criteria_result": null,
        "analysis": "The paper \"Xception: Deep Learning with Depthwise Separable Convolutions\" is highly relevant to the research query regarding novel neural network architectures for image recognition. It introduces a new architecture, Xception, which is based entirely on depthwise separable convolution layers. The paper details the architecture, compares it to Inception V3 on ImageNet and JFT datasets, and analyzes the impact of residual connections and intermediate activations. The methodology includes rigorous experimentation and comparison with existing state-of-the-art architectures. The venue (IEEE) aligns with the user's preference for computer science venues. The paper directly proposes and evaluates a novel architecture, making it highly relevant based on the ranking guidance.",
        "relevant_quotes": [
          "We propose a convolutional neural network architecture based entirely on depthwise separable convolution layers.",
          "Because this hypothesis is a stronger version of the hypothesis underlying the Inception architecture, we name our proposed architecture Xception, which stands for “Extreme Inception”.",
          "Compared to Inception V3, Xception shows small gains in classification performance on the ImageNet dataset and large gains on the JFT dataset."
        ]
      },
      {
        "title": "Swin Transformer V2: Scaling Up Capacity and Resolution",
        "authors": [
          "Liu Z.",
          "Hu H.",
          "Lin Y.",
          "Yao Z.",
          "Xie Z.",
          "Wei Y.",
          "Ning J.",
          "Cao Y.",
          "Zhang Z.",
          "Dong L.",
          "Wei F.",
          "Guo B."
        ],
        "year": 2022,
        "score": 0.7749999999999999,
        "extraction_result": null,
        "exclusion_criteria_result": null,
        "analysis": "The paper \"Swin Transformer V2: Scaling Up Capacity and Resolution\" introduces modifications to the Swin Transformer architecture to improve its scalability and performance, particularly regarding capacity and resolution. The core contributions are three adaptions on the original swin transformer architecture: 1) a res-post-norm to replace the previous pre-norm configuration; 2) a scaled cosine attention to replace the original dot product attention; 3) a log-spaced continuous relative position bias approach to replace the previous parameterized approach. These adaptions aim to address instability issues during training and improve transferability across different window resolutions, making it highly relevant to the query regarding novel neural network architectures for image recognition, especially given its focus on a transformer-based architecture and improvements to attention mechanisms. The paper presents empirical evidence of improved performance on image classification, object detection, semantic segmentation, and video action classification tasks. The limitations include a reliance on large datasets for pre-training and some architectural complexities introduced to handle memory consumption, which might limit accessibility for researchers with fewer resources. The paper is strongly aligned with the query, as it specifically focuses on proposing and evaluating a new architecture with significant modifications to existing transformer designs.",
        "relevant_quotes": [
          "To better scale up model capacity and window resolution, several adaptions are made on the original swin transformer architecture (v1): 1) a res-post-norm to replace the previous pre-norm configuration; 2) a scaled cosine attention to replace the original dot product attention; 3) a log-spaced continuous relative position bias approach to replace the previous parameterized approach.",
          "We have presented techniques for scaling Swin Trans-former up to 3 billion parameters and making it capable of training with images of up to 1,536x 1,536 resolution, including the res-post-norm and scaled cosine attention to make the model easier to be scaled up in capacity, as well a log -spaced continuous relative position bias approach which lets the model more effectively transferred across window resolutions.",
          "The adapted architecture is named Swin Transformer V2, and by scaling up capacity and resolution, it sets new records on 4 representative vision bench-marks."
        ]
      }
    ],
    "num_total_papers": 2,
    "timestamp": "2025-02-21T22:48:34.797427"
  }
}