Context extraction timestamp: 20240801_185535

<repository_structure>
<directory name="academic-claim-analyzer">
    <file>
        <name>.env</name>
        <path>.env</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>.gitignore</name>
        <path>.gitignore</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>LICENSE</name>
        <path>LICENSE</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>pyproject.toml</name>
        <path>pyproject.toml</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>README.md</name>
        <path>README.md</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>repo_context_extractor.py</name>
        <path>repo_context_extractor.py</path>
        <content>
import os
import datetime

EXCLUDED_DIRS = {".git", "__pycache__", "node_modules", ".venv"}
FULL_CONTENT_EXTENSIONS = {".py", ".dbml", ".yaml"}

def create_file_element(file_path, root_folder):
    relative_path = os.path.relpath(file_path, root_folder)
    file_name = os.path.basename(file_path)
    file_extension = os.path.splitext(file_name)[1]

    file_element = [
        f"    <file>\n        <name>{file_name}</name>\n        <path>{relative_path}</path>\n"
    ]

    if file_extension in FULL_CONTENT_EXTENSIONS:
        file_element.append("        <content>\n")
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                file_element.append(file.read())
        except UnicodeDecodeError:
            file_element.append("Binary or non-UTF-8 content not displayed")
        file_element.append("\n        </content>\n")
    else:
        file_element.append("        <content>Full content not provided</content>\n")

    file_element.append("    </file>\n")
    return "".join(file_element)

def get_repo_structure(root_folder):
    structure = ["<repository_structure>\n"]

    for subdir, dirs, files in os.walk(root_folder):
        dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]
        level = subdir.replace(root_folder, "").count(os.sep)
        indent = " " * 4 * level
        relative_subdir = os.path.relpath(subdir, root_folder)

        structure.append(f'{indent}<directory name="{os.path.basename(subdir)}">\n')
        for file in files:
            file_path = os.path.join(subdir, file)
            file_element = create_file_element(file_path, root_folder)
            structure.append(file_element)
        structure.append(f"{indent}</directory>\n")

    structure.append("</repository_structure>\n")
    return "".join(structure)

def main():
    root_folder = os.getcwd()  # Use the current working directory
    base_dir = os.path.basename(root_folder)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(root_folder, f"{base_dir}_context_{timestamp}.txt")

    # Delete the previous output file if it exists
    for file in os.listdir(root_folder):
        if file.startswith(f"{base_dir}_context_") and file.endswith(".txt"):
            os.remove(os.path.join(root_folder, file))
            print(f"Deleted previous context file: {file}")

    repo_structure = get_repo_structure(root_folder)

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"Context extraction timestamp: {timestamp}\n\n")
        f.write(repo_structure)

    print(f"Fresh repository context has been extracted to {output_file}")

if __name__ == "__main__":
    main()
        </content>
    </file>
    <file>
        <name>requirements.txt</name>
        <path>requirements.txt</path>
        <content>Full content not provided</content>
    </file>
</directory>
    <directory name=".pytest_cache">
    <file>
        <name>.gitignore</name>
        <path>.pytest_cache\.gitignore</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>CACHEDIR.TAG</name>
        <path>.pytest_cache\CACHEDIR.TAG</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>README.md</name>
        <path>.pytest_cache\README.md</path>
        <content>Full content not provided</content>
    </file>
    </directory>
        <directory name="v">
        </directory>
            <directory name="cache">
    <file>
        <name>lastfailed</name>
        <path>.pytest_cache\v\cache\lastfailed</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>nodeids</name>
        <path>.pytest_cache\v\cache\nodeids</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>stepwise</name>
        <path>.pytest_cache\v\cache\stepwise</path>
        <content>Full content not provided</content>
    </file>
            </directory>
    <directory name="academic_claim_analyzer">
    <file>
        <name>main.py</name>
        <path>academic_claim_analyzer\main.py</path>
        <content>
# src/academic_claim_analyzer/main.py
"""
Main orchestrator for the Academic Claim Analyzer.
"""

import asyncio
import logging
from typing import List, Type
from .query_formulator import formulate_queries
from .paper_scraper import scrape_papers
from .paper_ranker import rank_papers
from .search import OpenAlexSearch, ScopusSearch, CoreSearch, BaseSearch
from .models import ClaimAnalysis, RankedPaper

logger = logging.getLogger(__name__)

SEARCH_MODULES: List[Type[BaseSearch]] = [OpenAlexSearch, ScopusSearch, CoreSearch]

async def analyze_claim(
    claim: str,
    num_queries: int = 5,
    papers_per_query: int = 5,
    num_papers_to_return: int = 1
) -> ClaimAnalysis:
    """
    Analyze a given claim by searching for relevant papers, ranking them,
    and returning the top-ranked papers with supporting evidence.

    Args:
        claim (str): The claim to be analyzed.
        num_queries (int): Number of search queries to generate.
        papers_per_query (int): Number of papers to retrieve per query.
        num_papers_to_return (int): Number of top-ranked papers to return.

    Returns:
        ClaimAnalysis: Analysis result containing top-ranked papers with supporting evidence.
    """
    analysis = ClaimAnalysis(
        claim=claim,
        parameters={
            "num_queries": num_queries,
            "papers_per_query": papers_per_query,
            "num_papers_to_return": num_papers_to_return
        }
    )
    
    try:
        await _perform_analysis(analysis)
    except Exception as e:
        logger.error(f"Error during claim analysis: {str(e)}")
        analysis.metadata["error"] = str(e)
    
    return analysis

async def _perform_analysis(analysis: ClaimAnalysis) -> None:
    """
    Perform the actual analysis steps.
    """
    await _formulate_queries(analysis)
    await _perform_searches(analysis)
    await _scrape_papers(analysis)
    await _rank_papers(analysis)

async def _formulate_queries(analysis: ClaimAnalysis) -> None:
    """
    Formulate queries based on the claim.
    """
    queries = formulate_queries(analysis.claim, analysis.parameters["num_queries"])
    for query in queries:
        analysis.add_query(query, "formulator")

async def _perform_searches(analysis: ClaimAnalysis) -> None:
    """
    Perform searches using all search modules.
    """
    search_tasks = []
    for search_module_class in SEARCH_MODULES:
        search_module = search_module_class()
        for query in analysis.queries:
            search_tasks.append(_search_and_add_results(
                search_module, query.query, analysis.parameters["papers_per_query"], analysis
            ))
    await asyncio.gather(*search_tasks)

async def _search_and_add_results(search_module: BaseSearch, query: str, limit: int, analysis: ClaimAnalysis) -> None:
    """
    Perform a search and add results to the analysis.
    """
    try:
        results = await search_module.search(query, limit)
        for paper in results:
            analysis.add_search_result(paper)
    except Exception as e:
        logger.error(f"Error during search with {search_module.__class__.__name__}: {str(e)}")

async def _scrape_papers(analysis: ClaimAnalysis) -> None:
    """
    Scrape full text content for all papers in the analysis.
    """
    analysis.search_results = await scrape_papers(analysis.search_results)

async def _rank_papers(analysis: ClaimAnalysis) -> None:
    """
    Rank the papers based on relevance to the claim.
    """
    ranked_papers = await rank_papers(analysis.search_results, analysis.claim)
    for paper in ranked_papers:
        analysis.add_ranked_paper(paper)

async def main():
    claim = "Coffee consumption is associated with reduced risk of type 2 diabetes."
    analysis_result = await analyze_claim(claim)
    
    print(f"Claim: {analysis_result.claim}")
    print(f"Number of queries generated: {len(analysis_result.queries)}")
    print(f"Total papers found: {len(analysis_result.search_results)}")
    print(f"Number of ranked papers: {len(analysis_result.ranked_papers)}")
    print("\nTop ranked papers:")
    
    for paper in analysis_result.get_top_papers(analysis_result.parameters["num_papers_to_return"]):
        print(f"\nTitle: {paper.title}")
        print(f"Authors: {', '.join(paper.authors)}")
        print(f"DOI: {paper.doi}")
        print(f"Relevance Score: {paper.relevance_score}")
        print(f"Analysis: {paper.analysis}")
        print("Relevant Quotes:")
        for quote in paper.relevant_quotes:
            print(f"- {quote}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
        </content>
    </file>
    <file>
        <name>models.py</name>
        <path>academic_claim_analyzer\models.py</path>
        <content>
# src/academic_claim_analyzer/models.py

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from datetime import datetime

@dataclass
class SearchQuery:
    query: str
    source: str
    timestamp: datetime = field(default_factory=datetime.utcnow)

@dataclass
class Paper:
    title: str
    authors: List[str]
    year: int
    doi: str
    abstract: Optional[str] = None
    source: str = ""
    full_text: Optional[str] = None
    pdf_link: Optional[str] = None
    bibtex: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class RankedPaper(Paper):
    relevance_score: float
    relevant_quotes: List[str]
    analysis: str

@dataclass
class ClaimAnalysis:
    claim: str
    timestamp: datetime = field(default_factory=datetime.utcnow)
    parameters: Dict[str, Any] = field(default_factory=dict)
    queries: List[SearchQuery] = field(default_factory=list)
    search_results: List[Paper] = field(default_factory=list)
    ranked_papers: List[RankedPaper] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def add_query(self, query: str, source: str):
        self.queries.append(SearchQuery(query, source))

    def add_search_result(self, paper: Paper):
        self.search_results.append(paper)

    def add_ranked_paper(self, paper: RankedPaper):
        self.ranked_papers.append(paper)

    def get_top_papers(self, n: int) -> List[RankedPaper]:
        return sorted(self.ranked_papers, key=lambda x: x.relevance_score, reverse=True)[:n]
        </content>
    </file>
    <file>
        <name>paper_ranker.py</name>
        <path>academic_claim_analyzer\paper_ranker.py</path>
        <content>
# src/academic_claim_analyzer/paper_ranker.py

"""
This module contains the functions for ranking papers based on their relevance to a given claim.

The ranking algorithm is as follows:
To simplify the plan while maintaining output quality, we need to identify which components contribute the most to the robustness and accuracy of the results. We can streamline the process by focusing on the most impactful elements and eliminating steps that add complexity without significantly improving the outcome.

### Simplified Plan: Shuffled Group Ranking with Averaged Scoring

### Key Components to Retain:
1. **Clipping Papers**: Essential for ensuring manageable text lengths.
2. **Initial Group Comparisons**: Necessary for creating an initial ranking.
3. **Adaptive Shuffling**: Ensures diverse comparisons and reduces bias.
4. **Final Scoring and Selection**: Critical for determining the top papers.

### Components to Remove:
1. **Stratified Group Formation**: While helpful, the benefits of stratified sampling are marginal compared to random grouping when multiple rounds of shuffling are used.
2. **Weighted Scoring System**: The improvement from weighting scores is minimal compared to the complexity it adds. Simple averaging can suffice.
3. **Detailed Preprocessing for Relevant Sections**: Using the full text without detailed preprocessing for relevant sections simplifies implementation without a significant loss in quality, especially when using the full text within the 9,000-word limit.

### Final Simplified Plan:

#### Step-by-Step Process:

### Overview:
1. **Clipping Papers**: Ensure papers are clipped to a manageable length.
2. **Initial Group Comparisons**: Rank groups of papers.
3. **Adaptive Shuffling and Re-Comparison**: Shuffle and re-rank papers to ensure diverse comparisons.
4. **Final Selection**: Rank papers based on averaged scores and select the top N.

#### Step 1: Preprocessing - Clipping Papers

**Objective**: Reduce the length of each paper to a maximum of 9,000 words to ensure that the comparison text is manageable.

1. **Clip Papers**: 
   - Use a function to clip each paper to 9,000 words.
   - This ensures the text remains within the LLM's context window and focuses on the most relevant content.

#### Step 2: Initial Group Comparisons

**Objective**: Form initial groups of papers and rank them.

1. **Form Groups**:
   - Divide the clipped papers into groups of four.

2. **Comparison and Ranking**:
   - **Prompt**: "Rank these four papers from most to least relevant to the query: Paper A (summary/abstract), Paper B (summary/abstract), Paper C (summary/abstract), Paper D (summary/abstract)."
   - **Action**: The LLM ranks the four papers in one API call.
   - **Scoring**: Assign scores based on the rank within each group (4 points for the top paper, 3 for the second, 2 for the third, 1 for the fourth).

3. **Repeat for All Groups**:
   - Continue this process for all initial groups of four.

#### Step 3: Adaptive Shuffling and Re-Comparison

**Objective**: Ensure diverse comparisons by shuffling papers and re-ranking them in multiple rounds.

1. **Shuffle Papers**:
   - Randomly reshuffle the papers to form new groups, ensuring diversity and avoiding repeated groupings as much as possible.

2. **New Group Comparisons**:
   - Form new groups of four and repeat the comparison and ranking process.
   - Use the same ranking prompt as in Step 2.

3. **Repeat and Average**:
   - Perform multiple rounds of shuffling and comparisons (e.g., 3-5 rounds).
   - Average the scores for each paper across all rounds to get a final relevance score.

#### Step 4: Final Selection

**Objective**: Rank papers based on averaged scores and select the top N papers.

1. **Rank Based on Averaged Scores**:
   - Rank the papers based on their averaged scores from all comparison rounds.

2. **Select Top N Papers**:
   - Choose the top N papers based on the final ranking.

### Example Workflow for 20 Papers:

1. **Clipping Papers**:
   - Clip each of the 20 papers to a maximum of 9,000 words.

2. **Initial Group Comparisons**:
   - **Form Groups**: 20 papers, groups of 4 → 5 groups.
   - **API Calls**: 5 calls (each call ranks 4 papers).

3. **Shuffle and Re-Compare**:
   - **Shuffle**: Randomly reshuffle papers into new groups.
   - **Repeat Comparisons**: Perform 3 rounds of shuffling and comparisons.
   - **API Calls per Round**: 5 calls (each call ranks 4 papers).
   - **Total API Calls for Shuffling**: 3 rounds × 5 calls = 15 calls.

4. **Final Scoring and Selection**:
   - **Average Scores**: Aggregate and average scores from all rounds.
   - **Select Top Papers**: Based on final averaged scores.

### API Call Analysis for Different Numbers of Papers:

#### For 10 Papers:
- **Initial Group Comparisons**: \( \frac{10}{4} = 2.5 \) (round up to 3 groups) → 3 API calls.
- **Shuffling Rounds**: 3 rounds × 3 calls = 9 API calls.
- **Total**: 3 + 9 = 12 API calls.
- **Ratio**: 12 API calls / 10 papers = 1.2.

#### For 20 Papers:
- **Initial Group Comparisons**: \( \frac{20}{4} = 5 \) groups → 5 API calls.
- **Shuffling Rounds**: 3 rounds × 5 calls = 15 API calls.
- **Total**: 5 + 15 = 20 API calls.
- **Ratio**: 20 API calls / 20 papers = 1.

#### For 50 Papers:
- **Initial Group Comparisons**: \( \frac{50}{4} = 12.5 \) (round up to 13 groups) → 13 API calls.
- **Shuffling Rounds**: 3 rounds × 13 calls = 39 API calls.
- **Total**: 13 + 39 = 52 API calls.
- **Ratio**: 52 API calls / 50 papers = 1.04.

### Final Analysis:

**Pros**:
- **Efficiency**: Significant reduction in API calls compared to pairwise comparisons.
- **Fairness**: Shuffling and multiple rounds ensure that good and bad papers are fairly compared.
- **Precision**: Averaging scores across multiple rounds reduces the impact of any single comparison’s bias.

**Cons**:
- **Complexity**: Reduced from the original plan but still requires implementing shuffling logic.
- **API Cost**: Though reduced, still involves multiple rounds of comparisons.

### Conclusion:

By focusing on essential elements such as clipping papers, initial group comparisons, adaptive shuffling, and final scoring, we maintain a balance of efficiency, fairness, and precision while minimizing complexity and API calls. This streamlined approach should effectively rank academic papers with a high degree of reliability.


"""

from typing import List
from .models import Paper, RankedPaper

async def rank_papers(papers: List[Paper], claim: str) -> List[RankedPaper]:
    """
    Rank the given papers based on their relevance to the claim.

    Args:
        papers (List[Paper]): The papers to be ranked.
        claim (str): The claim to rank the papers against.

    Returns:
        List[RankedPaper]: A list of ranked papers with analysis and relevant quotes.
    """
    # Implementation details will be added later
    pass
        </content>
    </file>
    <file>
        <name>paper_scraper.py</name>
        <path>academic_claim_analyzer\paper_scraper.py</path>
        <content>
import asyncio
import random
import aiohttp
from playwright.async_api import async_playwright
from fake_useragent import UserAgent
import logging
import sys
import json
import fitz  # PyMuPDF
from bs4 import BeautifulSoup
import requests
from urllib.parse import urlparse, urljoin

class WebScraper:
    def __init__(self, session, max_concurrent_tasks=5):
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.user_agent = UserAgent()
        self.browser = None
        self.session = session
        self.logger = logging.getLogger(__name__)

    async def initialize(self):
        try:
            playwright = await async_playwright().start()
            self.browser = await playwright.chromium.launch(headless=True)
            self.logger.info("Browser initialized successfully")
        except Exception as e:
            self.logger.error(f"Failed to initialize browser: {str(e)}")
            raise

    async def close(self):
        if self.browser:
            await self.browser.close()
            self.logger.info("Browser closed")

    def normalize_url(self, url):
        if url.startswith("10."):
            return f"https://doi.org/{url}"
        elif url.startswith("doi:"):
            return f"https://doi.org/{url[4:]}"
        elif not url.startswith("http"):
            return f"https://{url}"
        return url

    async def scrape_url(self, url, max_retries=3):
        normalized_url = self.normalize_url(url)
        self.logger.info(f"Attempting to scrape URL: {normalized_url}")
        
        if normalized_url.lower().endswith(".pdf"):
            return await self.scrape_pdf(normalized_url, max_retries)
        else:
            return await self.scrape_web_page(normalized_url, max_retries)

    async def scrape_web_page(self, url, max_retries=3):
        if not self.browser:
            await self.initialize()

        retry_count = 0
        page = None
        while retry_count < max_retries:
            try:
                # Attempt with requests and BeautifulSoup first
                content = await self.extract_content_with_requests(url)
                if content and "You are accessing a machine-readable page" not in content and len(content.split()) > 200:
                    self.logger.info(f"Successfully scraped URL with requests: {url}")
                    return content

                context = await self.browser.new_context(
                    user_agent=self.user_agent.random,
                    viewport={"width": 1920, "height": 1080},
                    ignore_https_errors=True,
                    java_script_enabled=True,
                )

                await context.set_extra_http_headers({
                    "User-Agent": self.user_agent.random,
                    "Accept-Language": "en-US,en;q=0.9",
                    "Accept-Encoding": "gzip, deflate, br",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
                    "Connection": "keep-alive",
                    "DNT": "1",
                    "Upgrade-Insecure-Requests": "1",
                })

                page = await context.new_page()

                cookies = await self.load_cookies()
                if cookies:
                    await context.add_cookies(cookies)

                await self.navigate_to_url(page, url, max_retries=3)
                content = await self.extract_text_content(page)

                if not content:
                    self.logger.warning(f"No content extracted from {url}. Attempting to follow redirects.")
                    content = await self.follow_redirects(page, url)

                cookies = await context.cookies()
                await self.save_cookies(cookies)

                if content:
                    self.logger.info(f"Successfully scraped URL: {url}")
                    await context.close()
                    return content
                else:
                    raise Exception("No content extracted after following redirects")

            except Exception as e:
                self.logger.error(f"Error occurred while scraping URL: {url}. Error: {str(e)}")
                retry_count += 1
                await asyncio.sleep(random.uniform(1, 3))
            finally:
                if page:
                    try:
                        await page.close()
                    except Exception as e:
                        self.logger.warning(f"Error occurred while closing page: {str(e)}")

        self.logger.warning(f"Max retries exceeded for URL: {url}")
        return ""

    async def follow_redirects(self, page, original_url):
        try:
            current_url = await page.evaluate("window.location.href")
            if current_url != original_url:
                self.logger.info(f"Redirected from {original_url} to {current_url}")
                await self.navigate_to_url(page, current_url, max_retries=2)
                return await self.extract_text_content(page)
            return ""
        except Exception as e:
            self.logger.error(f"Error following redirects: {str(e)}")
            return ""

    async def scrape_pdf(self, url, max_retries=3):
        retry_count = 0
        while retry_count < max_retries:
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        pdf_bytes = await response.read()
                        pdf_text = self.extract_text_from_pdf(pdf_bytes)
                        if pdf_text:
                            self.logger.info(f"Successfully scraped PDF URL: {url}")
                            return pdf_text
                        else:
                            raise Exception("Failed to extract text from PDF")
                    else:
                        raise Exception(f"Failed to download PDF, status code: {response.status}")
            except Exception as e:
                self.logger.error(f"Error occurred while scraping PDF URL: {url}. Error: {str(e)}")
                retry_count += 1
                await asyncio.sleep(random.uniform(1, 3))
        self.logger.warning(f"Max retries exceeded for PDF URL: {url}")
        return ""

    def extract_text_from_pdf(self, pdf_bytes):
        try:
            document = fitz.open("pdf", pdf_bytes)
            text = ""
            for page in document:
                text += page.get_text()
            return text.strip()
        except Exception as e:
            self.logger.error(f"Failed to extract text from PDF. Error: {str(e)}")
            return ""

    async def extract_content_with_requests(self, url):
        try:
            response = requests.get(url, headers={"User-Agent": self.user_agent.random})
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, "html.parser")
                main_content = soup.find("div", id="abstract") or soup.find("main") or soup.find("body")
                if main_content:
                    for script in main_content(["script", "style"]):
                        script.decompose()
                    content_text = main_content.get_text(separator="\n", strip=True)
                    return content_text
            return ""
        except Exception as e:
            self.logger.error(f"Failed to extract content with requests. Error: {str(e)}")
            return ""

    async def get_url_content(self, url):
        async with self.semaphore:
            return await self.scrape_url(url)

    async def navigate_to_url(self, page, url, max_retries=3):
        retry_count = 0
        while retry_count < max_retries:
            try:
                response = await page.goto(url, wait_until="networkidle", timeout=60000)
                if response.ok:
                    await page.wait_for_load_state("load")
                    await asyncio.sleep(2)
                    return
                else:
                    raise Exception(f"Navigation failed with status: {response.status}")
            except Exception as e:
                self.logger.warning(f"Retrying URL: {url}. Remaining retries: {max_retries - retry_count - 1}. Error: {str(e)}")
                retry_count += 1
                await asyncio.sleep(random.uniform(1, 3))
        self.logger.error(f"Failed to navigate to URL: {url} after {max_retries} retries")
        raise Exception(f"Navigation failed for URL: {url}")

    async def extract_text_content(self, page):
        try:
            await page.wait_for_selector("body", timeout=10000)
            text_content = await page.evaluate("""
                () => {
                    const elements = document.querySelectorAll('p, h1, h2, h3, h4, h5, h6, li, td, th');
                    return Array.from(elements).map(element => element.innerText).join(' ');
                }
            """)
            return text_content.strip()
        except Exception as e:
            self.logger.error(f"Failed to extract text content. Error: {str(e)}")
            return ""

    async def save_cookies(self, cookies):
        with open("cookies.json", "w") as file:
            json.dump(cookies, file)

    async def load_cookies(self):
        try:
            with open("cookies.json", "r") as file:
                return json.load(file)
        except FileNotFoundError:
            return None

async def main():
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler("scraper.log"),
            logging.StreamHandler(sys.stdout),
        ],
    )

    async with aiohttp.ClientSession() as session:
        scraper = WebScraper(session=session)
        try:
            await scraper.initialize()
        except Exception as e:
            logging.error(f"Initialization failed: {e}")
            return

        urls = [
            "10.1016/j.ifacol.2020.12.237",
            "10.1016/j.agwat.2023.108536",
            "10.1016/j.atech.2023.100251",
            # ... (rest of the URLs)
        ]

        scrape_tasks = [asyncio.create_task(scraper.get_url_content(url)) for url in urls]
        scraped_contents = await asyncio.gather(*scrape_tasks)

        success_count = 0
        failure_count = 0

        print("\nScraping Results:\n" + "=" * 80)
        for url, content in zip(urls, scraped_contents):
            if content:
                first_1000_words = " ".join(content.split()[:1000])
                print(f"\nURL: {url}\nStatus: Success\nFirst 1000 words: {first_1000_words}\n" + "-" * 80)
                success_count += 1
            else:
                print(f"\nURL: {url}\nStatus: Failure\n" + "-" * 80)
                failure_count += 1

        print("\nSummary:\n" + "=" * 80)
        print(f"Total URLs scraped: {len(urls)}")
        print(f"Successful scrapes: {success_count}")
        print(f"Failed scrapes: {failure_count}")

        await scraper.close()

if __name__ == "__main__":
    asyncio.run(main())
        </content>
    </file>
    <file>
        <name>query_formulator.py</name>
        <path>academic_claim_analyzer\query_formulator.py</path>
        <content>
# src/academic_claim_analyzer/query_formulator.py

from typing import List
import json
from async_llm_handler import LLMHandler

SCOPUS_SEARCH_GUIDE = """
Syntax and Operators

Valid syntax for advanced search queries includes:

Field codes (e.g. TITLE, ABS, KEY, AUTH, AFFIL) to restrict searches to specific parts of documents
Boolean operators (AND, OR, AND NOT) to combine search terms
Proximity operators (W/n, PRE/n) to find words within a specified distance - W/n: Finds terms within "n" words of each other, regardless of order. Example: journal W/15 publishing finds articles where "journal" and "publishing" are within two words of each other. - PRE/n: Finds terms in the specified order and within "n" words of each other. Example: data PRE/50 analysis finds articles where "data" appears before "analysis" within three words. - To find terms in the same sentence, use 15. To find terms in the same paragraph, use 50 -
Quotation marks for loose/approximate phrase searches
Braces {} for exact phrase searches
Wildcards (*) to capture variations of search terms
Invalid syntax includes:

Mixing different proximity operators (e.g. W/n and PRE/n) in the same expression
Using wildcards or proximity operators with exact phrase searches
Placing AND NOT before other Boolean operators
Using wildcards on their own without any search terms
Ideal Search Structure

An ideal advanced search query should:

Use field codes to focus the search on the most relevant parts of documents
Combine related concepts using AND and OR
Exclude irrelevant terms with AND NOT at the end
Employ quotation marks and braces appropriately for phrase searching
Include wildcards to capture variations of key terms (while avoiding mixing them with other operators)
Follow the proper order of precedence for operators
Complex searches should be built up systematically, with parentheses to group related expressions as needed. The information from the provided documents on syntax rules and operators should be applied rigorously.

** Critical: all double quotes other than the outermost ones should be preceded by a backslash (") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **

Example Advanced Searches

[
"TITLE-ABS-KEY(("precision agriculture" OR "precision farming") AND ("machine learning" OR "AI") AND "water")",
"TITLE-ABS-KEY((iot OR \"internet of things\") AND (irrigation OR watering) AND sensor*)",
"TITLE-ABS-Key((\"precision farming\" OR \"precision agriculture\") AND (\"deep learning\" OR \"neural networks\") AND \"water\")",
"TITLE-ABS-KEY((crop W/5 monitor*) AND \"remote sensing\" AND (irrigation OR water*))",
"TITLE(\"precision irrigation\" OR \"variable rate irrigation\" AND \"machine learning\")"
]


** Critical: all double quotes other than the outermost ones should be preceded by a backslash (") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **. 

These example searches demonstrate different ways to effectively combine key concepts related to precision agriculture, irrigation, real-time monitoring, IoT, machine learning and related topics using advanced search operators. They make use of field codes, Boolean and proximity operators, phrase searching, and wildcards to construct targeted, comprehensive searches to surface the most relevant research. The topic focus is achieved through carefully chosen search terms covering the desired themes.
"""

OPENALEX_SEARCH_GUIDE = """
Syntax and Operators
Valid syntax for advanced alex search queries includes:
Using quotation marks %22%22 for exact phrase matches
Adding a minus sign - before terms to exclude them
Employing the OR operator in all caps to find pages containing either term
Using the site%3A operator to limit results to a specific website
Applying the filetype%3A operator to find specific file formats like PDF, DOC, etc.
Adding the * wildcard as a placeholder for unknown words
`
Invalid syntax includes:
Putting a plus sign + before words (alex stopped supporting this)
Using other special characters like %3F, %24, %26, %23, etc. within search terms
Explicitly using the AND operator (alex's default behavior makes it redundant)

Ideal Search Structure
An effective alex search query should:
Start with the most important search terms
Use specific, descriptive keywords related to irrigation scheduling, management, and precision irrigation
Utilize exact phrases in %22quotes%22 for specific word combinations
Exclude irrelevant terms using the - minus sign
Connect related terms or synonyms with OR
Apply the * wildcard strategically for flexibility
Note:

By following these guidelines and using proper URL encoding, you can construct effective and accurate search queries for alex.

Searches should be concise yet precise, following the syntax rules carefully. 

Example Searches
[
"https://api.openalex.org/works?search=%22precision+irrigation%22+%2B%22soil+moisture+sensors%22+%2B%22irrigation+scheduling%22&sort=relevance_score:desc&per-page=30",
"https://api.openalex.org/works?search=%22machine+learning%22+%2B%22irrigation+management%22+%2B%22crop+water+demand+prediction%22&sort=relevance_score:desc&per-page=30",
"https://api.openalex.org/works?search=%22IoT+sensors%22+%2B%22real-time%22+%2B%22soil+moisture+monitoring%22+%2B%22crop+water+stress%22&sort=relevance_score:desc&per-page=30",
"https://api.openalex.org/works?search=%22remote+sensing%22+%2B%22vegetation+indices%22+%2B%22irrigation+scheduling%22&sort=relevance_score:desc&per-page=30",
"https://api.openalex.org/works?search=%22wireless+sensor+networks%22+%2B%22precision+agriculture%22+%2B%22variable+rate+irrigation%22+%2B%22irrigation+automation%22&sort=relevance_score:desc&per-page=30"
]

These example searches demonstrate how to create targeted, effective alex searches. They focus on specific topics, exclude irrelevant results, allow synonym flexibility, and limit to relevant domains when needed. The search terms are carefully selected to balance relevance and specificity while avoiding being overly restrictive.  By combining relevant keywords, exact phrases, and operators, these searches help generate high-quality results for the given topics.
"""

GENERATE_QUERIES = """
You are tasked with generating optimized search queries to find relevant research articles addressing a specific point. Follow these instructions carefully:

1. Review the following point that needs to be addressed by the literature search:
<point_content>
{POINT_CONTENT}
</point_content>

2. Consider the following search guidance:
<search_guidance>
{SEARCH_GUIDANCE}
</search_guidance>

3. Generate {NUM_QUERIES} highly optimized search queries that would surface the most relevant, insightful, and comprehensive set of research articles to shed light on various aspects of the given point. Your queries should:

- Directly address the key issues and nuances of the point content
- Demonstrate creativity and variety to capture different dimensions of the topic
- Use precise terminology and logical operators for high-quality results
- Cover a broad range of potential subtopics, perspectives, and article types related to the point
- Strictly adhere to any specific requirements provided in the search guidance

4. Provide your response as a list of strings in the following format:

[
  "query_1",
  "query_2",
  "query_3",
  ...
]

Replace query_1, query_2, etc. with your actual search queries. The number of queries should match {NUM_QUERIES}.

5. If the search guidance specifies a particular platform (e.g., Scopus, Web of Science), ensure your queries are formatted appropriately for that platform.

6. Important: If your queries contain quotation marks, ensure they are properly escaped with a backslash (\") to maintain valid list formatting.

Generate the list of search queries now, following the instructions above.
"""




async def formulate_queries(claim: str, num_queries: int, query_type: str) -> List[str]:
    """
    Generate search queries based on the given claim.

    Args:
        claim (str): The claim to generate queries for.
        num_queries (int): The number of queries to generate.
        query_type (str): The type of query to generate ('scopus' or 'openalex').

    Returns:
        List[str]: A list of generated search queries.
    """
    handler = LLMHandler()

    if query_type.lower() == 'scopus':
        search_guidance = SCOPUS_SEARCH_GUIDE
    elif query_type.lower() == 'openalex':
        search_guidance = OPENALEX_SEARCH_GUIDE
    else:
        raise ValueError(f"Unsupported query type: {query_type}")

    prompt = GENERATE_QUERIES.format(
        point_content=claim,
        search_guidance=search_guidance,
        num_queries=num_queries,
        query_type=query_type
    )

    response = await handler.query(prompt, model="gpt_4o_mini", sync=False)
    
    try:
        queries = json.loads(response)
        if not isinstance(queries, list) or len(queries) != num_queries:
            raise ValueError("Invalid response format")
        return queries
    except json.JSONDecodeError:
        raise ValueError("Failed to parse the response as JSON")
        </content>
    </file>
    <file>
        <name>__init__.py</name>
        <path>academic_claim_analyzer\__init__.py</path>
        <content>
# src/academic_claim_analyzer/__init__.py
"""
Academic Claim Analyzer

This package provides functionality to analyze academic claims by searching
for relevant papers, ranking them, and providing supporting evidence.
"""

from .main import analyze_claim
        </content>
    </file>
    </directory>
        <directory name="search">
    <file>
        <name>base.py</name>
        <path>academic_claim_analyzer\search\base.py</path>
        <content>
# src/academic_claim_analyzer/search/base.py

from abc import ABC, abstractmethod
from typing import List
from ..models import Paper

class BaseSearch(ABC):
    @abstractmethod
    async def search(self, query: str, limit: int) -> List[Paper]:
        """
        Perform a search using the given query and return a list of search results.

        Args:
            query (str): The search query.
            limit (int): The maximum number of results to return.

        Returns:
            List[Paper]: A list of search results.
        """
        pass
        </content>
    </file>
    <file>
        <name>bibtex.py</name>
        <path>academic_claim_analyzer\search\bibtex.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>core_search.py</name>
        <path>academic_claim_analyzer\search\core_search.py</path>
        <content>
# src/academic_claim_analyzer/search/core_search.py

import aiohttp
import os
from typing import List
from dotenv import load_dotenv
from .base import BaseSearch
from ..models import Paper
import logging

logger = logging.getLogger(__name__)

load_dotenv()

class CORESearch(BaseSearch):
    def __init__(self):
        self.api_key = os.getenv("CORE_API_KEY")
        if not self.api_key:
            raise ValueError("CORE_API_KEY not found in environment variables")
        self.base_url = "https://api.core.ac.uk/v3"

    async def search(self, query: str, limit: int) -> List[Paper]:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Accept": "application/json",
        }

        params = {
            "q": query,
            "limit": limit,
        }

        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(f"{self.base_url}/search/works", headers=headers, json=params) as response:
                    if response.status == 200:
                        logger.info("CORE API request successful.")
                        data = await response.json()
                        return self._parse_results(data)
                    else:
                        logger.error(f"CORE API request failed with status code: {response.status}")
                        return []
            except Exception as e:
                logger.error(f"Error occurred while making CORE API request: {str(e)}")
                return []

    def _parse_results(self, data: dict) -> List[Paper]:
        results = []
        for entry in data.get("results", []):
            result = Paper(
                doi=entry.get("doi", ""),
                title=entry.get("title", ""),
                authors=[author["name"] for author in entry.get("authors", [])],
                year=entry.get("publicationYear", 0),
                abstract=entry.get("abstract", ""),
                pdf_link=entry.get("downloadUrl", ""),
                source=entry.get("publisher", ""),
                full_text=entry.get("fullText", ""),
                metadata={
                    "citation_count": entry.get("citationCount", 0),
                    "core_id": entry.get("id", "")
                }
            )
            results.append(result)
        return results
        </content>
    </file>
    <file>
        <name>openalex_search.py</name>
        <path>academic_claim_analyzer\search\openalex_search.py</path>
        <content>
# src/academic_claim_analyzer/search/openalex_search.py

import aiohttp
import asyncio
import urllib.parse
from typing import List
from .base import BaseSearch
from ..models import Paper
import logging

logger = logging.getLogger(__name__)

class OpenAlexSearch(BaseSearch):
    def __init__(self, email: str):
        self.base_url = "https://api.openalex.org"
        self.email = email
        self.semaphore = asyncio.Semaphore(5)  # Limit to 5 concurrent requests

    async def search(self, query: str, limit: int) -> List[Paper]:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=600)) as session:
            encoded_query = urllib.parse.quote(query)
            search_url = f"{self.base_url}/works?search={encoded_query}&per_page={limit}&mailto={self.email}"

            async with self.semaphore:
                try:
                    await asyncio.sleep(0.2)  # Rate limiting
                    async with session.get(search_url) as response:
                        if response.status == 200:
                            data = await response.json()
                            return self._parse_results(data)
                        else:
                            logger.error(f"Unexpected status code from OpenAlex API: {response.status}")
                            return []
                except Exception as e:
                    logger.error(f"Error occurred while making request to OpenAlex API: {str(e)}")
                    return []

    def _parse_results(self, data: dict) -> List[Paper]:
        results = []
        for work in data.get("results", []):
            result = Paper(
                doi=work.get("doi", ""),
                title=work.get("title", ""),
                authors=[author["author"]["display_name"] for author in work.get("authorships", [])],
                year=work.get("publication_year", 0),
                abstract=work.get("abstract"),
                pdf_link=work.get("primary_location", {}).get("pdf_url"),
                source=work.get("primary_location", {}).get("source", {}).get("display_name", ""),
                metadata={
                    "citation_count": work.get("cited_by_count", 0),
                    "openalex_id": work.get("id", "")
                }
            )
            results.append(result)
        return results
        </content>
    </file>
    <file>
        <name>scopus_search.py</name>
        <path>academic_claim_analyzer\search\scopus_search.py</path>
        <content>
# src/academic_claim_analyzer/search/scopus_search.py

import aiohttp
import asyncio
import os
from typing import List
from collections import deque
import time
from dotenv import load_dotenv
from .base import BaseSearch
from ..models import Paper
import logging

logger = logging.getLogger(__name__)

load_dotenv()

class ScopusSearch(BaseSearch):
    def __init__(self):
        self.api_key = os.getenv("SCOPUS_API_KEY")
        if not self.api_key:
            raise ValueError("SCOPUS_API_KEY not found in environment variables")
        self.base_url = "http://api.elsevier.com/content/search/scopus"
        self.request_times = deque(maxlen=6)
        self.semaphore = asyncio.Semaphore(5)  # Limit to 5 concurrent requests

    async def search(self, query: str, limit: int) -> List[Paper]:
        headers = {
            "X-ELS-APIKey": self.api_key,
            "Accept": "application/json",
        }

        params = {
            "query": query,
            "count": limit,
            "view": "COMPLETE",
        }

        async with aiohttp.ClientSession() as session:
            async with self.semaphore:
                try:
                    # Ensure compliance with the rate limit
                    await self._wait_for_rate_limit()

                    async with session.get(self.base_url, headers=headers, params=params) as response:
                        if response.status == 200:
                            data = await response.json()
                            return self._parse_results(data)
                        else:
                            logger.error(f"Scopus API request failed with status code: {response.status}")
                            return []
                except Exception as e:
                    logger.error(f"Error occurred while making Scopus API request: {str(e)}")
                    return []

    async def _wait_for_rate_limit(self):
        while True:
            current_time = time.time()
            if not self.request_times or current_time - self.request_times[0] >= 1:
                self.request_times.append(current_time)
                break
            else:
                await asyncio.sleep(0.2)

    def _parse_results(self, data: dict) -> List[Paper]:
        results = []
        for entry in data.get("search-results", {}).get("entry", []):
            result = Paper(
                doi=entry.get("prism:doi", ""),
                title=entry.get("dc:title", ""),
                authors=[author.get("authname", "") for author in entry.get("author", [])],
                year=int(entry.get("prism:coverDate", "").split("-")[0]),
                abstract=entry.get("dc:description", ""),
                pdf_link="",  # Scopus API doesn't provide direct PDF links
                source=entry.get("prism:publicationName", ""),
                metadata={
                    "citation_count": int(entry.get("citedby-count", 0)),
                    "scopus_id": entry.get("dc:identifier", ""),
                    "eid": entry.get("eid", "")
                }
            )
            results.append(result)
        return results
        </content>
    </file>
    <file>
        <name>__init__.py</name>
        <path>academic_claim_analyzer\search\__init__.py</path>
        <content>
from .base import BaseSearch
from .core_search import CORESearch
from .openalex_search import OpenAlexSearch
from .scopus_search import ScopusSearch
        </content>
    </file>
        </directory>
    <directory name="tests">
    <file>
        <name>conftest.py</name>
        <path>tests\conftest.py</path>
        <content>
# tests\conftest.py



        </content>
    </file>
    <file>
        <name>test_claim_analysis_e2e.py</name>
        <path>tests\test_claim_analysis_e2e.py</path>
        <content>
# tests\test_claim_analysis_e2e.py



        </content>
    </file>
    <file>
        <name>test_main.py</name>
        <path>tests\test_main.py</path>
        <content>
# tests/test_main.py

import pytest
from unittest.mock import patch, MagicMock
from academic_claim_analyzer.main import analyze_claim
from academic_claim_analyzer.models import ClaimAnalysis, Paper, RankedPaper

@pytest.fixture
def mock_search_results():
    return [
        Paper(
            doi="10.1016/j.diabres.2023.03.050",
            title="Coffee consumption and risk of type 2 diabetes: An updated meta-analysis of prospective cohort studies",
            authors=["Jiali Zheng", "Jingjing Zhang", "Yong Zhou"],
            year=2023,
            abstract="This meta-analysis of prospective cohort studies suggests that coffee consumption is associated with a reduced risk of type 2 diabetes, with the strongest effect observed for 3-4 cups per day.",
            source="Diabetes Research and Clinical Practice"
        ),
        Paper(
            doi="10.2337/dc20-1800",
            title="Long-term coffee consumption, caffeine metabolism genetics, and risk of cardiovascular disease: a prospective analysis of up to 347,077 individuals and 8368 cases",
            authors=["Licia Iacoviello", "Marialaura Bonaccio", "Augusto Di Castelnuovo"],
            year=2021,
            abstract="This large prospective study suggests that coffee consumption is associated with a lower risk of cardiovascular disease, with the relationship influenced by caffeine metabolism genetics.",
            source="Diabetes Care"
        )
    ]

@pytest.fixture
def mock_ranked_papers():
    return [
        RankedPaper(
            doi="10.1016/j.diabres.2023.03.050",
            title="Coffee consumption and risk of type 2 diabetes: An updated meta-analysis of prospective cohort studies",
            authors=["Jiali Zheng", "Jingjing Zhang", "Yong Zhou"],
            year=2023,
            abstract="This meta-analysis of prospective cohort studies suggests that coffee consumption is associated with a reduced risk of type 2 diabetes, with the strongest effect observed for 3-4 cups per day.",
            source="Diabetes Research and Clinical Practice",
            relevance_score=0.95,
            relevant_quotes=["coffee consumption is associated with a reduced risk of type 2 diabetes"],
            analysis="This meta-analysis provides strong evidence supporting the claim that coffee consumption is associated with reduced risk of type 2 diabetes."
        ),
        RankedPaper(
            doi="10.2337/dc20-1800",
            title="Long-term coffee consumption, caffeine metabolism genetics, and risk of cardiovascular disease: a prospective analysis of up to 347,077 individuals and 8368 cases",

            authors=["Licia Iacoviello", "Marialaura Bonaccio", "Augusto Di Castelnuovo"],
            year=2021,
            abstract="This large prospective study suggests that coffee consumption is associated with a lower risk of cardiovascular disease, with the relationship influenced by caffeine metabolism genetics.",
            source="Diabetes Care",
            relevance_score=0.75,
            relevant_quotes=["coffee consumption is associated with a lower risk of cardiovascular disease"],
            analysis="While this study focuses on cardiovascular disease, it provides indirect support for the potential health benefits of coffee consumption, which may be relevant to the claim about type 2 diabetes risk reduction."
        )
    ]

@pytest.mark.asyncio
async def test_analyze_claim(mock_search_results, mock_ranked_papers):
    with patch('src.academic_claim_analyzer.query_formulator.formulate_queries') as mock_formulate_queries, \
         patch('src.academic_claim_analyzer.search.OpenAlexSearch.search') as mock_search, \
         patch('src.academic_claim_analyzer.paper_scraper.scrape_papers') as mock_scrape_papers, \
         patch('src.academic_claim_analyzer.paper_ranker.rank_papers') as mock_rank_papers:

        mock_formulate_queries.return_value = [
            "TITLE-ABS-KEY(coffee AND consumption AND (\"type 2 diabetes\" OR \"diabetes mellitus\") AND risk)",
            "TITLE-ABS-KEY(\"coffee intake\" AND \"diabetes risk\" AND (meta-analysis OR \"systematic review\"))"
        ]
        mock_search.return_value = mock_search_results
        mock_scrape_papers.return_value = mock_search_results
        mock_rank_papers.return_value = mock_ranked_papers

        claim = "Coffee consumption is associated with reduced risk of type 2 diabetes."
        result = await analyze_claim(claim, num_queries=2, papers_per_query=2, num_papers_to_return=2)

        assert isinstance(result, ClaimAnalysis)
        assert result.claim == claim
        assert len(result.queries) == 2
        assert len(result.search_results) == 4  # 2 queries * 2 papers per query
        assert len(result.ranked_papers) == 2

        top_papers = result.get_top_papers(2)
        assert len(top_papers) == 2
        assert top_papers[0].relevance_score >= top_papers[1].relevance_score
        assert "meta-analysis" in top_papers[0].title.lower()
        assert "type 2 diabetes" in top_papers[0].title.lower()

@pytest.mark.asyncio
async def test_analyze_claim_error_handling():
    with patch('src.academic_claim_analyzer.query_formulator.formulate_queries', side_effect=Exception("API rate limit exceeded")):
        claim = "Mindfulness meditation can help reduce symptoms of anxiety and depression."
        result = await analyze_claim(claim)
        assert isinstance(result, ClaimAnalysis)
        assert result.claim == claim
        assert "error" in result.metadata
        assert result.metadata["error"] == "API rate limit exceeded"
        assert len(result.queries) == 0
        assert len(result.search_results) == 0
        assert len(result.ranked_papers) == 0

@pytest.mark.asyncio
async def test_analyze_claim_no_results():
    with patch('src.academic_claim_analyzer.query_formulator.formulate_queries') as mock_formulate_queries, \
         patch('src.academic_claim_analyzer.search.OpenAlexSearch.search') as mock_search:

        mock_formulate_queries.return_value = ["TITLE-ABS-KEY(non_existent_topic AND improbable_research)"]
        mock_search.return_value = []

        claim = "Non-existent topic is related to improbable research outcomes."
        result = await analyze_claim(claim, num_queries=1, papers_per_query=5, num_papers_to_return=2)

        assert isinstance(result, ClaimAnalysis)
        assert result.claim == claim
        assert len(result.queries) == 1
        assert len(result.search_results) == 0
        assert len(result.ranked_papers) == 0
        assert "No relevant papers found" in result.metadata.get("analysis", "")

@pytest.mark.asyncio
async def test_analyze_claim_partial_results():
    with patch('src.academic_claim_analyzer.query_formulator.formulate_queries') as mock_formulate_queries, \
         patch('src.academic_claim_analyzer.search.OpenAlexSearch.search') as mock_search, \
         patch('src.academic_claim_analyzer.paper_scraper.scrape_papers') as mock_scrape_papers, \
         patch('src.academic_claim_analyzer.paper_ranker.rank_papers') as mock_rank_papers:

        mock_formulate_queries.return_value = [
            "TITLE-ABS-KEY(exercise AND \"cardiovascular health\" AND \"older adults\")",
            "TITLE-ABS-KEY(\"physical activity\" AND \"heart disease\" AND elderly)"
        ]
        mock_search.side_effect = [mock_search_results[:1], []]  # First query returns one result, second query returns no results
        mock_scrape_papers.return_value = mock_search_results[:1]
        mock_rank_papers.return_value = mock_ranked_papers[:1]

        claim = "Regular exercise is linked to improved cardiovascular health in older adults."
        result = await analyze_claim(claim, num_queries=2, papers_per_query=2, num_papers_to_return=2)

        assert isinstance(result, ClaimAnalysis)
        assert result.claim == claim
        assert len(result.queries) == 2
        assert len(result.search_results) == 1
        assert len(result.ranked_papers) == 1
        assert "Partial results found" in result.metadata.get("analysis", "")

if __name__ == "__main__":
    pytest.main()
        </content>
    </file>
    <file>
        <name>test_paper_ranker.py</name>
        <path>tests\test_paper_ranker.py</path>
        <content>
# tests\test_paper_ranker.py



        </content>
    </file>
    <file>
        <name>test_paper_scraper.py</name>
        <path>tests\test_paper_scraper.py</path>
        <content>
# tests\test_paper_scraper.py



        </content>
    </file>
    <file>
        <name>test_query_formulator.py</name>
        <path>tests\test_query_formulator.py</path>
        <content>
# tests/test_query_formulator.py

import pytest
from academic_claim_analyzer.query_formulator import formulate_queries


@pytest.mark.asyncio
@pytest.mark.parametrize("claim, num_queries, query_type, expected_keywords", [
    (
        "Coffee consumption is associated with reduced risk of type 2 diabetes.",
        3,
        "scopus",
        ["coffee", "consumption", "reduced risk", "type 2 diabetes", "association"]
    ),
    (
        "Mindfulness meditation can help reduce symptoms of anxiety and depression.",
        4,
        "openalex",
        ["mindfulness", "meditation", "anxiety", "depression", "symptoms", "reduce"]
    ),
    (
        "Regular exercise is linked to improved cardiovascular health in older adults.",
        5,
        "scopus",
        ["regular exercise", "cardiovascular health", "older adults", "linked", "improved"]
    ),
])
async def test_formulate_queries(claim, num_queries, query_type, expected_keywords):
    queries = await formulate_queries(claim, num_queries, query_type)
    assert isinstance(queries, list)
    assert len(queries) == num_queries
    for query in queries:
        assert isinstance(query, str)
        assert len(query) > 0
        assert any(keyword.lower() in query.lower() for keyword in expected_keywords)

    # Check if queries are different from each other
    assert len(set(queries)) == num_queries

    # Check if queries adhere to the specified query type format
    if query_type.lower() == 'scopus':
        assert all("TITLE-ABS-KEY" in query for query in queries)
    elif query_type.lower() == 'openalex':
        assert all(query.startswith('"') or query.startswith('-') or query.startswith('+') for query in queries)

@pytest.mark.asyncio
async def test_formulate_queries_invalid_query_type():
    with pytest.raises(ValueError, match="Unsupported query type"):
        await formulate_queries("Test claim", 3, "invalid_type")

@pytest.mark.asyncio
async def test_formulate_queries_error_handling():
    # Test with invalid JSON response (simulated by mocking the LLMHandler)
    with pytest.patch('src.academic_claim_analyzer.query_formulator.LLMHandler') as mock_handler:
        mock_handler.return_value.query.return_value = "Invalid JSON"
        with pytest.raises(ValueError, match="Failed to parse the response as JSON"):
            await formulate_queries("Test claim", 3, "scopus")

    # Test with valid JSON but incorrect format
    with pytest.patch('src.academic_claim_analyzer.query_formulator.LLMHandler') as mock_handler:
        mock_handler.return_value.query.return_value = '{"key": "value"}'
        with pytest.raises(ValueError, match="Invalid response format"):
            await formulate_queries("Test claim", 3, "scopus")
        </content>
    </file>
    <file>
        <name>test_scrape_rank_integration.py</name>
        <path>tests\test_scrape_rank_integration.py</path>
        <content>
# tests\test_scrape_rank_integration.py



        </content>
    </file>
    <file>
        <name>test_search_integration.py</name>
        <path>tests\test_search_integration.py</path>
        <content>
# tests\test_search_integration.py



        </content>
    </file>
    </directory>
        <directory name="test_search">
    <file>
        <name>test_core.py</name>
        <path>tests\test_search\test_core.py</path>
        <content>
# tests/test_search/test_core.py

import pytest
from unittest.mock import patch, MagicMock
from academic_claim_analyzer.search.base import BaseSearch
from academic_claim_analyzer.search.core_search import CORESearch
from academic_claim_analyzer.models import Paper

@pytest.fixture
def mock_core_response():
    return {
        "results": [
            {
                "doi": "10.1161/CIRCULATIONAHA.120.050775",
                "title": "Physical Activity and Cardiovascular Health in Older Adults: A Comprehensive Review",
                "authors": [
                    {"name": "Jennifer L. Carter"},
                    {"name": "Robert A. Thompson"},
                    {"name": "Lisa M. Brown"}
                ],
                "publicationYear": 2021,
                "abstract": "This comprehensive review examines the relationship between regular physical activity and cardiovascular health in older adults. The evidence strongly supports that engagement in regular exercise is associated with improved cardiovascular outcomes, including reduced risk of heart disease, stroke, and mortality.",
                "downloadUrl": "https://www.ahajournals.org/doi/pdf/10.1161/CIRCULATIONAHA.120.050775",
                "publisher": "Circulation",
                "fullText": "This is the full text of the paper, including detailed methods, results, and discussion...",
                "citationCount": 45,
                "id": "core:98765432"
            }
        ]
    }

@pytest.mark.asyncio
async def test_core_search(mock_core_response):
    with patch('aiohttp.ClientSession.post') as mock_post:
        mock_post.return_value.__aenter__.return_value.status = 200
        mock_post.return_value.__aenter__.return_value.json = MagicMock(return_value=mock_core_response)

        with patch.dict('os.environ', {'CORE_API_KEY': 'fake_api_key'}):
            search = CORESearch()
            results = await search.search("physical activity cardiovascular health older adults", 1)

            assert len(results) == 1
            paper = results[0]
            assert isinstance(paper, Paper)
            assert paper.doi == "10.1161/CIRCULATIONAHA.120.050775"
            assert paper.title == "Physical Activity and Cardiovascular Health in Older Adults: A Comprehensive Review"
            assert paper.authors == ["Jennifer L. Carter", "Robert A. Thompson", "Lisa M. Brown"]
            assert paper.year == 2021
            assert "relationship between regular physical activity and cardiovascular health in older adults" in paper.abstract
            assert paper.pdf_link == "https://www.ahajournals.org/doi/pdf/10.1161/CIRCULATIONAHA.120.050775"
            assert paper.source == "Circulation"
            assert "This is the full text of the paper" in paper.full_text
            assert paper.metadata["citation_count"] == 45
            assert paper.metadata["core_id"] == "core:98765432"

@pytest.mark.asyncio
async def test_core_search_error():
    with patch('aiohttp.ClientSession.post') as mock_post:
        mock_post.return_value.__aenter__.return_value.status = 500

        with patch.dict('os.environ', {'CORE_API_KEY': 'fake_api_key'}):
            search = CORESearch()
            results = await search.search("physical activity cardiovascular health older adults", 1)

            assert len(results) == 0
        </content>
    </file>
    <file>
        <name>test_openalex.py</name>
        <path>tests\test_search\test_openalex.py</path>
        <content>
# tests/test_search/test_openalex.py

import pytest
from unittest.mock import patch, MagicMock
from academic_claim_analyzer.search.openalex_search import OpenAlexSearch
from academic_claim_analyzer.models import Paper

@pytest.fixture
def mock_openalex_response():
    return {
        "results": [
            {
                "doi": "10.1016/j.diabres.2023.03.050",
                "title": "Coffee consumption and risk of type 2 diabetes: An updated meta-analysis of prospective cohort studies",
                "authorships": [
                    {"author": {"display_name": "Jiali Zheng"}},
                    {"author": {"display_name": "Jingjing Zhang"}},
                    {"author": {"display_name": "Yong Zhou"}}
                ],
                "publication_year": 2023,
                "abstract": "This meta-analysis of prospective cohort studies suggests that coffee consumption is associated with a reduced risk of type 2 diabetes, with the strongest effect observed for 3-4 cups per day.",
                "primary_location": {
                    "pdf_url": "https://www.sciencedirect.com/science/article/pii/S0168822723001512/pdf",
                    "source": {"display_name": "Diabetes Research and Clinical Practice"}
                },
                "cited_by_count": 12,
                "id": "W4235689012"
            }
        ]
    }

@pytest.mark.asyncio
async def test_openalex_search(mock_openalex_response):
    with patch('aiohttp.ClientSession.get') as mock_get:
        mock_get.return_value.__aenter__.return_value.status = 200
        mock_get.return_value.__aenter__.return_value.json = MagicMock(return_value=mock_openalex_response)

        search = OpenAlexSearch(email="researcher@university.edu")
        results = await search.search("coffee consumption type 2 diabetes", 1)

        assert len(results) == 1
        paper = results[0]
        assert isinstance(paper, Paper)
        assert paper.doi == "10.1016/j.diabres.2023.03.050"
        assert paper.title == "Coffee consumption and risk of type 2 diabetes: An updated meta-analysis of prospective cohort studies"
        assert paper.authors == ["Jiali Zheng", "Jingjing Zhang", "Yong Zhou"]
        assert paper.year == 2023
        assert "coffee consumption is associated with a reduced risk of type 2 diabetes" in paper.abstract
        assert paper.pdf_link == "https://www.sciencedirect.com/science/article/pii/S0168822723001512/pdf"
        assert paper.source == "Diabetes Research and Clinical Practice"
        assert paper.metadata["citation_count"] == 12
        assert paper.metadata["openalex_id"] == "W4235689012"

@pytest.mark.asyncio
async def test_openalex_search_error():
    with patch('aiohttp.ClientSession.get') as mock_get:
        mock_get.return_value.__aenter__.return_value.status = 500

        search = OpenAlexSearch(email="researcher@university.edu")
        results = await search.search("coffee consumption type 2 diabetes", 1)

        assert len(results) == 0
        </content>
    </file>
    <file>
        <name>test_scopus.py</name>
        <path>tests\test_search\test_scopus.py</path>
        <content>
# tests/test_search/test_scopus.py

import pytest
from unittest.mock import patch, MagicMock
from academic_claim_analyzer.search.scopus_search import ScopusSearch
from academic_claim_analyzer.models import Paper

@pytest.fixture
def mock_scopus_response():
    return {
        "search-results": {
            "entry": [
                {
                    "prism:doi": "10.1016/j.jad.2022.01.053",
                    "dc:title": "Mindfulness-based interventions for anxiety and depression in adults: A meta-analysis of randomized controlled trials",
                    "author": [
                        {"authname": "Sarah J. Goldberg"},
                        {"authname": "Michael A. Smith"},
                        {"authname": "Emily R. Johnson"}
                    ],
                    "prism:coverDate": "2022-04-15",
                    "dc:description": "This meta-analysis provides evidence that mindfulness-based interventions are effective in reducing symptoms of anxiety and depression in adults, with moderate effect sizes observed across various clinical and non-clinical populations.",
                    "prism:publicationName": "Journal of Affective Disorders",
                    "citedby-count": "87",
                    "dc:identifier": "SCOPUS_ID:85123456789",
                    "eid": "2-s2.0-85123456789"
                }
            ]
        }
    }

@pytest.mark.asyncio
async def test_scopus_search(mock_scopus_response):
    with patch('aiohttp.ClientSession.get') as mock_get:
        mock_get.return_value.__aenter__.return_value.status = 200
        mock_get.return_value.__aenter__.return_value.json = MagicMock(return_value=mock_scopus_response)

        with patch.dict('os.environ', {'SCOPUS_API_KEY': 'fake_api_key'}):
            search = ScopusSearch()
            results = await search.search("mindfulness anxiety depression meta-analysis", 1)

            assert len(results) == 1
            paper = results[0]
            assert isinstance(paper, Paper)
            assert paper.doi == "10.1016/j.jad.2022.01.053"
            assert paper.title == "Mindfulness-based interventions for anxiety and depression in adults: A meta-analysis of randomized controlled trials"
            assert paper.authors == ["Sarah J. Goldberg", "Michael A. Smith", "Emily R. Johnson"]
            assert paper.year == 2022
            assert "mindfulness-based interventions are effective in reducing symptoms of anxiety and depression" in paper.abstract
            assert paper.source == "Journal of Affective Disorders"
            assert paper.metadata["citation_count"] == 87
            assert paper.metadata["scopus_id"] == "SCOPUS_ID:85123456789"
            assert paper.metadata["eid"] == "2-s2.0-85123456789"

@pytest.mark.asyncio
async def test_scopus_search_error():
    with patch('aiohttp.ClientSession.get') as mock_get:
        mock_get.return_value.__aenter__.return_value.status = 500

        with patch.dict('os.environ', {'SCOPUS_API_KEY': 'fake_api_key'}):
            search = ScopusSearch()
            results = await search.search("mindfulness anxiety depression meta-analysis", 1)

            assert len(results) == 0
        </content>
    </file>
        </directory>
</repository_structure>
