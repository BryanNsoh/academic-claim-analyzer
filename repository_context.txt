<repository_structure>
<directory name="academic-claim-analyzer">
    <file>
        <name>.gitignore</name>
        <path>.gitignore</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>LICENSE</name>
        <path>LICENSE</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>pyproject.toml</name>
        <path>pyproject.toml</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>README.md</name>
        <path>README.md</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>repo_context_extractor.py</name>
        <path>repo_context_extractor.py</path>
        <content>
import os

EXCLUDED_DIRS = {".git", "__pycache__", "node_modules", ".venv"}
FULL_CONTENT_EXTENSIONS = {".py", ".dbml", ".yaml"}

def create_file_element(file_path, root_folder):
    relative_path = os.path.relpath(file_path, root_folder)
    file_name = os.path.basename(file_path)
    file_extension = os.path.splitext(file_name)[1]

    file_element = [
        f"    <file>\n        <name>{file_name}</name>\n        <path>{relative_path}</path>\n"
    ]

    if file_extension in FULL_CONTENT_EXTENSIONS:
        file_element.append("        <content>\n")
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                file_element.append(file.read())
        except UnicodeDecodeError:
            file_element.append("Binary or non-UTF-8 content not displayed")
        file_element.append("\n        </content>\n")
    else:
        file_element.append("        <content>Full content not provided</content>\n")

    file_element.append("    </file>\n")
    return "".join(file_element)

def get_repo_structure(root_folder):
    structure = ["<repository_structure>\n"]

    for subdir, dirs, files in os.walk(root_folder):
        dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]
        level = subdir.replace(root_folder, "").count(os.sep)
        indent = " " * 4 * level
        relative_subdir = os.path.relpath(subdir, root_folder)

        structure.append(f'{indent}<directory name="{os.path.basename(subdir)}">\n')
        for file in files:
            file_path = os.path.join(subdir, file)
            file_element = create_file_element(file_path, root_folder)
            structure.append(file_element)
        structure.append(f"{indent}</directory>\n")

    structure.append("</repository_structure>\n")
    return "".join(structure)

def main():
    root_folder = os.getcwd()  # Use the current working directory
    output_file = os.path.join(root_folder, "repository_context.txt")

    # Delete the previous output file if it exists
    if os.path.exists(output_file):
        os.remove(output_file)
        print(f"Deleted previous {output_file}")

    repo_structure = get_repo_structure(root_folder)

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(repo_structure)

    print(f"Fresh repository context has been extracted to {output_file}")

if __name__ == "__main__":
    main()
        </content>
    </file>
</directory>
    <directory name="src">
    </directory>
        <directory name="academic_claim_analyzer">
    <file>
        <name>main.py</name>
        <path>src\academic_claim_analyzer\main.py</path>
        <content>
# src/academic_claim_analyzer/main.py
"""
Main orchestrator for the Academic Claim Analyzer.
"""

import asyncio
from typing import List, Dict
from .query_formulator import formulate_queries
from .paper_scraper import scrape_papers
from .paper_ranker import rank_papers
from .search import OpenAlexSearch, ScopusSearch, CoreSearch
from .models import RankedPaper

async def analyze_claim(
    claim: str,
    num_queries: int = 5,
    papers_per_query: int = 5,
    num_papers_to_return: int = 1
) -> List[RankedPaper]:
    """
    Analyze a given claim by searching for relevant papers, ranking them,
    and returning the top-ranked papers with supporting evidence.

    Args:
        claim (str): The claim to be analyzed.
        num_queries (int): Number of search queries to generate.
        papers_per_query (int): Number of papers to retrieve per query.
        num_papers_to_return (int): Number of top-ranked papers to return.

    Returns:
        List[RankedPaper]: List of top-ranked papers with supporting evidence.
    """
    # Formulate queries
    queries = formulate_queries(claim, num_queries)
    
    # Perform searches
    search_modules = [OpenAlexSearch(), ScopusSearch(), CoreSearch()]
    all_papers = []
    for search_module in search_modules:
        for query in queries:
            results = await search_module.search(query, papers_per_query)
            all_papers.extend(results)
    
    # Scrape full text
    scraped_papers = await scrape_papers(all_papers)
    
    # Rank papers
    ranked_papers = await rank_papers(scraped_papers, claim)
    
    # Return top N papers
    return ranked_papers[:num_papers_to_return]

if __name__ == "__main__":
    claim = "Coffee consumption is associated with reduced risk of type 2 diabetes."
    results = asyncio.run(analyze_claim(claim))
    for paper in results:
        print(f"Title: {paper.title}")
        print(f"Authors: {', '.join(paper.authors)}")
        print(f"DOI: {paper.doi}")
        print(f"Paper Rank: {paper.rank}")
        print(f"Analysis: {paper.analysis}")
        print("Relevant Quotes:")
        for quote in paper.relevant_quotes:
            print(f"- {quote}")
        print("\n")
        </content>
    </file>
    <file>
        <name>models.py</name>
        <path>src\academic_claim_analyzer\models.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>paper_ranker.py</name>
        <path>src\academic_claim_analyzer\paper_ranker.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>paper_scraper.py</name>
        <path>src\academic_claim_analyzer\paper_scraper.py</path>
        <content>
import asyncio
import random
import aiohttp
from playwright.async_api import async_playwright
from fake_useragent import UserAgent
import logging
import sys
import json
import fitz  # PyMuPDF
from bs4 import BeautifulSoup
import requests
from urllib.parse import urlparse, urljoin

class WebScraper:
    def __init__(self, session, max_concurrent_tasks=5):
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.user_agent = UserAgent()
        self.browser = None
        self.session = session
        self.logger = logging.getLogger(__name__)

    async def initialize(self):
        try:
            playwright = await async_playwright().start()
            self.browser = await playwright.chromium.launch(headless=True)
            self.logger.info("Browser initialized successfully")
        except Exception as e:
            self.logger.error(f"Failed to initialize browser: {str(e)}")
            raise

    async def close(self):
        if self.browser:
            await self.browser.close()
            self.logger.info("Browser closed")

    def normalize_url(self, url):
        if url.startswith("10."):
            return f"https://doi.org/{url}"
        elif url.startswith("doi:"):
            return f"https://doi.org/{url[4:]}"
        elif not url.startswith("http"):
            return f"https://{url}"
        return url

    async def scrape_url(self, url, max_retries=3):
        normalized_url = self.normalize_url(url)
        self.logger.info(f"Attempting to scrape URL: {normalized_url}")
        
        if normalized_url.lower().endswith(".pdf"):
            return await self.scrape_pdf(normalized_url, max_retries)
        else:
            return await self.scrape_web_page(normalized_url, max_retries)

    async def scrape_web_page(self, url, max_retries=3):
        if not self.browser:
            await self.initialize()

        retry_count = 0
        page = None
        while retry_count < max_retries:
            try:
                # Attempt with requests and BeautifulSoup first
                content = await self.extract_content_with_requests(url)
                if content and "You are accessing a machine-readable page" not in content and len(content.split()) > 200:
                    self.logger.info(f"Successfully scraped URL with requests: {url}")
                    return content

                context = await self.browser.new_context(
                    user_agent=self.user_agent.random,
                    viewport={"width": 1920, "height": 1080},
                    ignore_https_errors=True,
                    java_script_enabled=True,
                )

                await context.set_extra_http_headers({
                    "User-Agent": self.user_agent.random,
                    "Accept-Language": "en-US,en;q=0.9",
                    "Accept-Encoding": "gzip, deflate, br",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
                    "Connection": "keep-alive",
                    "DNT": "1",
                    "Upgrade-Insecure-Requests": "1",
                })

                page = await context.new_page()

                cookies = await self.load_cookies()
                if cookies:
                    await context.add_cookies(cookies)

                await self.navigate_to_url(page, url, max_retries=3)
                content = await self.extract_text_content(page)

                if not content:
                    self.logger.warning(f"No content extracted from {url}. Attempting to follow redirects.")
                    content = await self.follow_redirects(page, url)

                cookies = await context.cookies()
                await self.save_cookies(cookies)

                if content:
                    self.logger.info(f"Successfully scraped URL: {url}")
                    await context.close()
                    return content
                else:
                    raise Exception("No content extracted after following redirects")

            except Exception as e:
                self.logger.error(f"Error occurred while scraping URL: {url}. Error: {str(e)}")
                retry_count += 1
                await asyncio.sleep(random.uniform(1, 3))
            finally:
                if page:
                    try:
                        await page.close()
                    except Exception as e:
                        self.logger.warning(f"Error occurred while closing page: {str(e)}")

        self.logger.warning(f"Max retries exceeded for URL: {url}")
        return ""

    async def follow_redirects(self, page, original_url):
        try:
            current_url = await page.evaluate("window.location.href")
            if current_url != original_url:
                self.logger.info(f"Redirected from {original_url} to {current_url}")
                await self.navigate_to_url(page, current_url, max_retries=2)
                return await self.extract_text_content(page)
            return ""
        except Exception as e:
            self.logger.error(f"Error following redirects: {str(e)}")
            return ""

    async def scrape_pdf(self, url, max_retries=3):
        retry_count = 0
        while retry_count < max_retries:
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        pdf_bytes = await response.read()
                        pdf_text = self.extract_text_from_pdf(pdf_bytes)
                        if pdf_text:
                            self.logger.info(f"Successfully scraped PDF URL: {url}")
                            return pdf_text
                        else:
                            raise Exception("Failed to extract text from PDF")
                    else:
                        raise Exception(f"Failed to download PDF, status code: {response.status}")
            except Exception as e:
                self.logger.error(f"Error occurred while scraping PDF URL: {url}. Error: {str(e)}")
                retry_count += 1
                await asyncio.sleep(random.uniform(1, 3))
        self.logger.warning(f"Max retries exceeded for PDF URL: {url}")
        return ""

    def extract_text_from_pdf(self, pdf_bytes):
        try:
            document = fitz.open("pdf", pdf_bytes)
            text = ""
            for page in document:
                text += page.get_text()
            return text.strip()
        except Exception as e:
            self.logger.error(f"Failed to extract text from PDF. Error: {str(e)}")
            return ""

    async def extract_content_with_requests(self, url):
        try:
            response = requests.get(url, headers={"User-Agent": self.user_agent.random})
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, "html.parser")
                main_content = soup.find("div", id="abstract") or soup.find("main") or soup.find("body")
                if main_content:
                    for script in main_content(["script", "style"]):
                        script.decompose()
                    content_text = main_content.get_text(separator="\n", strip=True)
                    return content_text
            return ""
        except Exception as e:
            self.logger.error(f"Failed to extract content with requests. Error: {str(e)}")
            return ""

    async def get_url_content(self, url):
        async with self.semaphore:
            return await self.scrape_url(url)

    async def navigate_to_url(self, page, url, max_retries=3):
        retry_count = 0
        while retry_count < max_retries:
            try:
                response = await page.goto(url, wait_until="networkidle", timeout=60000)
                if response.ok:
                    await page.wait_for_load_state("load")
                    await asyncio.sleep(2)
                    return
                else:
                    raise Exception(f"Navigation failed with status: {response.status}")
            except Exception as e:
                self.logger.warning(f"Retrying URL: {url}. Remaining retries: {max_retries - retry_count - 1}. Error: {str(e)}")
                retry_count += 1
                await asyncio.sleep(random.uniform(1, 3))
        self.logger.error(f"Failed to navigate to URL: {url} after {max_retries} retries")
        raise Exception(f"Navigation failed for URL: {url}")

    async def extract_text_content(self, page):
        try:
            await page.wait_for_selector("body", timeout=10000)
            text_content = await page.evaluate("""
                () => {
                    const elements = document.querySelectorAll('p, h1, h2, h3, h4, h5, h6, li, td, th');
                    return Array.from(elements).map(element => element.innerText).join(' ');
                }
            """)
            return text_content.strip()
        except Exception as e:
            self.logger.error(f"Failed to extract text content. Error: {str(e)}")
            return ""

    async def save_cookies(self, cookies):
        with open("cookies.json", "w") as file:
            json.dump(cookies, file)

    async def load_cookies(self):
        try:
            with open("cookies.json", "r") as file:
                return json.load(file)
        except FileNotFoundError:
            return None

async def main():
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler("scraper.log"),
            logging.StreamHandler(sys.stdout),
        ],
    )

    async with aiohttp.ClientSession() as session:
        scraper = WebScraper(session=session)
        try:
            await scraper.initialize()
        except Exception as e:
            logging.error(f"Initialization failed: {e}")
            return

        urls = [
            "10.1016/j.ifacol.2020.12.237",
            "10.1016/j.agwat.2023.108536",
            "10.1016/j.atech.2023.100251",
            # ... (rest of the URLs)
        ]

        scrape_tasks = [asyncio.create_task(scraper.get_url_content(url)) for url in urls]
        scraped_contents = await asyncio.gather(*scrape_tasks)

        success_count = 0
        failure_count = 0

        print("\nScraping Results:\n" + "=" * 80)
        for url, content in zip(urls, scraped_contents):
            if content:
                first_1000_words = " ".join(content.split()[:1000])
                print(f"\nURL: {url}\nStatus: Success\nFirst 1000 words: {first_1000_words}\n" + "-" * 80)
                success_count += 1
            else:
                print(f"\nURL: {url}\nStatus: Failure\n" + "-" * 80)
                failure_count += 1

        print("\nSummary:\n" + "=" * 80)
        print(f"Total URLs scraped: {len(urls)}")
        print(f"Successful scrapes: {success_count}")
        print(f"Failed scrapes: {failure_count}")

        await scraper.close()

if __name__ == "__main__":
    asyncio.run(main())
        </content>
    </file>
    <file>
        <name>query_formulator.py</name>
        <path>src\academic_claim_analyzer\query_formulator.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>__init__.py</name>
        <path>src\academic_claim_analyzer\__init__.py</path>
        <content>
# src/academic_claim_analyzer/__init__.py
"""
Academic Claim Analyzer

This package provides functionality to analyze academic claims by searching
for relevant papers, ranking them, and providing supporting evidence.
"""

from .main import analyze_claim
        </content>
    </file>
        </directory>
            <directory name="search">
    <file>
        <name>base.py</name>
        <path>src\academic_claim_analyzer\search\base.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>bibtex.py</name>
        <path>src\academic_claim_analyzer\search\bibtex.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>core.py</name>
        <path>src\academic_claim_analyzer\search\core.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>openalex.py</name>
        <path>src\academic_claim_analyzer\search\openalex.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>scopus.py</name>
        <path>src\academic_claim_analyzer\search\scopus.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>__init__.py</name>
        <path>src\academic_claim_analyzer\search\__init__.py</path>
        <content>

        </content>
    </file>
            </directory>
    <directory name="tests">
    <file>
        <name>test_main.py</name>
        <path>tests\test_main.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>test_paper_ranker.py</name>
        <path>tests\test_paper_ranker.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>test_paper_scraper.py</name>
        <path>tests\test_paper_scraper.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>test_query_formulator.py</name>
        <path>tests\test_query_formulator.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>__init__.py</name>
        <path>tests\__init__.py</path>
        <content>

        </content>
    </file>
    </directory>
        <directory name="test_search">
    <file>
        <name>test_core.py</name>
        <path>tests\test_search\test_core.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>test_openalex.py</name>
        <path>tests\test_search\test_openalex.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>test_scopus.py</name>
        <path>tests\test_search\test_scopus.py</path>
        <content>

        </content>
    </file>
    <file>
        <name>__init__.py</name>
        <path>tests\test_search\__init__.py</path>
        <content>

        </content>
    </file>
        </directory>
</repository_structure>
